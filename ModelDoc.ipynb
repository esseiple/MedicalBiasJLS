{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "# conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate', 'address','ssn', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    \n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # MERGE DATASETS\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    \n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    # split into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Y column to predict is diabetes\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9067532372253403\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9195847547778879\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9144492131616596\n",
      "best score overall is:  0.9195847547778879  with model:  RFC\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# keep track of best Logistic Regression Score\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "DTCScore  = grid_search.best_score_\n",
    "bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Random Forrest Classifier    \n",
    "forrest = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "RFCScore  = grid_search.best_score_\n",
    "bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "#SVC\n",
    "SVM = SVC()\n",
    "\n",
    "# use grid search to find best gamma for SVM\n",
    "g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "print(\"best LR :\", LRScore)\n",
    "print(\"best DTC:\", DTCScore)\n",
    "print(\"best max depth: \", bestDTCDepth)\n",
    "print(\"best RFC: \", RFCScore)\n",
    "print(\"best max depth: \", bestRFCDepth)\n",
    "print(\"best SVM: \", SVMScore)\n",
    "\n",
    "max_score = 0\n",
    "max_model = \"\"\n",
    "if LRScore > max_score:\n",
    "    max_score = LRScore\n",
    "    max_model = \"LR\"\n",
    "if DTCScore > max_score:\n",
    "    max_score = DTCScore\n",
    "    max_model = \"DTC\"\n",
    "if RFCScore > max_score:\n",
    "    max_score = RFCScore\n",
    "    max_model = \"RFC\"\n",
    "if SVMScore > max_score:\n",
    "    max_score = SVMScore\n",
    "    max_model = \"SVM\"\n",
    "\n",
    "print(\"best score overall is: \", max_score, \" with model: \", max_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would compute risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asian', 1: 'black', 2: 'hispanic', 3: 'white'}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define average risk score finding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Asian patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.480179786835909"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Black patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2382530898722196"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Hispanic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3263173407187264"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(2, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for white patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3169193686398422"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(3, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'F', 1: 'M'}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3705511053037141"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26757624516308265"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.714533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.578034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.497687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.433813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.426106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.396401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.373317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.334757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.317450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.309996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.309242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.297601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.290970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.285537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.268832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.245826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.242375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.201640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.186933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.004461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.714533\n",
       "13            polish  0.578034\n",
       "9             german  0.497687\n",
       "12           mexican  0.433813\n",
       "1           american  0.426106\n",
       "14        portuguese  0.396401\n",
       "6            english  0.373317\n",
       "17          scottish  0.334757\n",
       "11           italian  0.317450\n",
       "5          dominican  0.309996\n",
       "15      puerto_rican  0.309242\n",
       "3   central_american  0.297601\n",
       "0            african  0.290970\n",
       "7             french  0.285537\n",
       "8    french_canadian  0.268832\n",
       "4            chinese  0.245826\n",
       "18           swedish  0.242375\n",
       "16           russian  0.201640\n",
       "10             irish  0.186933\n",
       "19       west_indian  0.004461"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep taking people from the top and bottom until its 20 on each side, then find av risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n",
    "                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n",
    "                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n",
    "\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble appending town names to dataframe with town codes and people counts\n",
    "\n",
    "need that so that we can then get the top twenty of each \n",
    "to then average to get rich likelihood and poor likelihood of diabetes \n",
    "yayyyy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df with all the information for teh rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info(town, bp_code_swapped, townCounts_df):\n",
    "    town_full = f'{town} MA US'\n",
    "    code = bp_code_swapped[town_full]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df['birthplace'] == code].empty:\n",
    "        count = townCounts_df[townCounts_df['birthplace'] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {'birthplace': town_full, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n",
    "\n",
    "townCounts_df = pd.merge(X_test, birthplace_counts, on='birthplace')\n",
    "town_info_rich = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "town_info_poor = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "\n",
    "bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "for town in richTowns:\n",
    "    \n",
    "    new_row_df = find_town_info(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "for town in poorTowns:\n",
    "    \n",
    "    new_row_df = find_town_info(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed with the following part to get top 65 people from each rich and poor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_towns_by_sum_pop(town_info):\n",
    "    \n",
    "    townsUsed = set()\n",
    "    peopleCount = 0\n",
    "\n",
    "    for index, row in town_info.iterrows():\n",
    "        \n",
    "        if peopleCount > 65:\n",
    "            break\n",
    "        \n",
    "        birthplace = row['birthplace']\n",
    "        count = row['count']\n",
    "        townsUsed.add(birthplace)\n",
    "        peopleCount += count\n",
    "    \n",
    "    return townsUsed, peopleCount\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich)\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_town_codes = []\n",
    "\n",
    "for town_full in richTownsUsed:\n",
    "    rich_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3328582558648457"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = X_test['birthplace'].isin(rich_town_codes)\n",
    "prob_subset = pred_prob[indices]\n",
    "av_rich_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "av_rich_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_town_codes = []\n",
    "\n",
    "for town_full in poorTownsUsed:\n",
    "    poor_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3212567199706269"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = X_test['birthplace'].isin(poor_town_codes)\n",
    "prob_subset = pred_prob[indices]\n",
    "av_poor_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "av_poor_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really annoying result..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
