{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "# conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate', 'address','ssn', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    \n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # MERGE DATASETS\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    \n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    # split into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Y column to predict is diabetes\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9076079380800411\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9195847547778879\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9144492131616596\n",
      "best score overall is:  0.9195847547778879  with model:  RFC\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# keep track of best Logistic Regression Score\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "DTCScore  = grid_search.best_score_\n",
    "bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Random Forrest Classifier    \n",
    "forrest = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "RFCScore  = grid_search.best_score_\n",
    "bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "#SVC\n",
    "SVM = SVC()\n",
    "\n",
    "# use grid search to find best gamma for SVM\n",
    "g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "print(\"best LR :\", LRScore)\n",
    "print(\"best DTC:\", DTCScore)\n",
    "print(\"best max depth: \", bestDTCDepth)\n",
    "print(\"best RFC: \", RFCScore)\n",
    "print(\"best max depth: \", bestRFCDepth)\n",
    "print(\"best SVM: \", SVMScore)\n",
    "\n",
    "max_score = 0\n",
    "max_model = \"\"\n",
    "if LRScore > max_score:\n",
    "    max_score = LRScore\n",
    "    max_model = \"LR\"\n",
    "if DTCScore > max_score:\n",
    "    max_score = DTCScore\n",
    "    max_model = \"DTC\"\n",
    "if RFCScore > max_score:\n",
    "    max_score = RFCScore\n",
    "    max_model = \"RFC\"\n",
    "if SVMScore > max_score:\n",
    "    max_score = SVMScore\n",
    "    max_model = \"SVM\"\n",
    "\n",
    "print(\"best score overall is: \", max_score, \" with model: \", max_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would compute risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asian', 1: 'black', 2: 'hispanic', 3: 'white'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define average risk score finding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Asian patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4802564988116467"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Black patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23767625364135003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Hispanic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3262324676115449"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(2, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for white patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31505264823741713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(3, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'F', 1: 'M'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36977009192535165"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2655602960849451"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'african',\n",
       " 1: 'american',\n",
       " 2: 'asian_indian',\n",
       " 3: 'central_american',\n",
       " 4: 'chinese',\n",
       " 5: 'dominican',\n",
       " 6: 'english',\n",
       " 7: 'french',\n",
       " 8: 'french_canadian',\n",
       " 9: 'german',\n",
       " 10: 'irish',\n",
       " 11: 'italian',\n",
       " 12: 'mexican',\n",
       " 13: 'polish',\n",
       " 14: 'portuguese',\n",
       " 15: 'puerto_rican',\n",
       " 16: 'russian',\n",
       " 17: 'scottish',\n",
       " 18: 'swedish',\n",
       " 19: 'west_indian'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.715143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.576223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.492006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.432326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.423334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.369974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.334637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.317963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.312373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.309286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.298240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.288455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.286428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.262199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.245370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.238256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.202594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.184378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.003162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.715143\n",
       "13            polish  0.576223\n",
       "9             german  0.492006\n",
       "12           mexican  0.432326\n",
       "1           american  0.423334\n",
       "14        portuguese  0.397959\n",
       "6            english  0.369974\n",
       "17          scottish  0.334637\n",
       "11           italian  0.317963\n",
       "5          dominican  0.312373\n",
       "15      puerto_rican  0.309286\n",
       "3   central_american  0.298240\n",
       "0            african  0.288455\n",
       "7             french  0.286428\n",
       "8    french_canadian  0.262199\n",
       "4            chinese  0.245370\n",
       "18           swedish  0.238256\n",
       "16           russian  0.202594\n",
       "10             irish  0.184378\n",
       "19       west_indian  0.003162"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n",
    "\n",
    "av_risk_eth_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Abington MA US',\n",
       " 1: 'Acton MA US',\n",
       " 2: 'Acushnet MA US',\n",
       " 3: 'Adams MA US',\n",
       " 4: 'Agawam Town MA US',\n",
       " 5: 'Alford MA US',\n",
       " 6: 'Amesbury Town MA US',\n",
       " 7: 'Amherst MA US',\n",
       " 8: 'Andover MA US',\n",
       " 9: 'Arlington MA US',\n",
       " 10: 'Ashburnham MA US',\n",
       " 11: 'Ashby MA US',\n",
       " 12: 'Ashfield MA US',\n",
       " 13: 'Ashland MA US',\n",
       " 14: 'Athol MA US',\n",
       " 15: 'Attleboro MA US',\n",
       " 16: 'Auburn MA US',\n",
       " 17: 'Avon MA US',\n",
       " 18: 'Barnstable Town MA US',\n",
       " 19: 'Barre MA US',\n",
       " 20: 'Becket MA US',\n",
       " 21: 'Bedford MA US',\n",
       " 22: 'Bellingham MA US',\n",
       " 23: 'Belmont MA US',\n",
       " 24: 'Berkley MA US',\n",
       " 25: 'Beverly MA US',\n",
       " 26: 'Billerica MA US',\n",
       " 27: 'Boston MA US',\n",
       " 28: 'Bourne MA US',\n",
       " 29: 'Boxborough MA US',\n",
       " 30: 'Boxford MA US',\n",
       " 31: 'Boylston MA US',\n",
       " 32: 'Braintree Town MA US',\n",
       " 33: 'Brewster MA US',\n",
       " 34: 'Bridgewater MA US',\n",
       " 35: 'Brockton MA US',\n",
       " 36: 'Brookfield MA US',\n",
       " 37: 'Brookline MA US',\n",
       " 38: 'Burlington MA US',\n",
       " 39: 'Cambridge MA US',\n",
       " 40: 'Canton MA US',\n",
       " 41: 'Carlisle MA US',\n",
       " 42: 'Carver MA US',\n",
       " 43: 'Charlemont MA US',\n",
       " 44: 'Charlton MA US',\n",
       " 45: 'Chatham MA US',\n",
       " 46: 'Chelmsford MA US',\n",
       " 47: 'Chelsea MA US',\n",
       " 48: 'Chicopee MA US',\n",
       " 49: 'Chilmark MA US',\n",
       " 50: 'Clinton MA US',\n",
       " 51: 'Cohasset MA US',\n",
       " 52: 'Colrain MA US',\n",
       " 53: 'Concord MA US',\n",
       " 54: 'Dalton MA US',\n",
       " 55: 'Danvers MA US',\n",
       " 56: 'Dartmouth MA US',\n",
       " 57: 'Dedham MA US',\n",
       " 58: 'Deerfield MA US',\n",
       " 59: 'Dennis MA US',\n",
       " 60: 'Douglas MA US',\n",
       " 61: 'Dover MA US',\n",
       " 62: 'Dracut MA US',\n",
       " 63: 'Dudley MA US',\n",
       " 64: 'Duxbury MA US',\n",
       " 65: 'East Bridgewater MA US',\n",
       " 66: 'East Brookfield MA US',\n",
       " 67: 'East Longmeadow MA US',\n",
       " 68: 'Eastham MA US',\n",
       " 69: 'Easthampton Town MA US',\n",
       " 70: 'Easton MA US',\n",
       " 71: 'Everett MA US',\n",
       " 72: 'Fairhaven MA US',\n",
       " 73: 'Fall River MA US',\n",
       " 74: 'Falmouth MA US',\n",
       " 75: 'Fitchburg MA US',\n",
       " 76: 'Florida MA US',\n",
       " 77: 'Foxborough MA US',\n",
       " 78: 'Framingham MA US',\n",
       " 79: 'Franklin Town MA US',\n",
       " 80: 'Freetown MA US',\n",
       " 81: 'Gardner MA US',\n",
       " 82: 'Gill MA US',\n",
       " 83: 'Gloucester MA US',\n",
       " 84: 'Grafton MA US',\n",
       " 85: 'Granby MA US',\n",
       " 86: 'Granville MA US',\n",
       " 87: 'Great Barrington MA US',\n",
       " 88: 'Greenfield Town MA US',\n",
       " 89: 'Groton MA US',\n",
       " 90: 'Groveland MA US',\n",
       " 91: 'Hadley MA US',\n",
       " 92: 'Hamilton MA US',\n",
       " 93: 'Hampden MA US',\n",
       " 94: 'Hancock MA US',\n",
       " 95: 'Hanover MA US',\n",
       " 96: 'Hanson MA US',\n",
       " 97: 'Hardwick MA US',\n",
       " 98: 'Harwich MA US',\n",
       " 99: 'Haverhill MA US',\n",
       " 100: 'Hingham MA US',\n",
       " 101: 'Holbrook MA US',\n",
       " 102: 'Holden MA US',\n",
       " 103: 'Holliston MA US',\n",
       " 104: 'Holyoke MA US',\n",
       " 105: 'Hopedale MA US',\n",
       " 106: 'Hopkinton MA US',\n",
       " 107: 'Hudson MA US',\n",
       " 108: 'Hull MA US',\n",
       " 109: 'Huntington MA US',\n",
       " 110: 'Ipswich MA US',\n",
       " 111: 'Kingston MA US',\n",
       " 112: 'Lancaster MA US',\n",
       " 113: 'Lawrence MA US',\n",
       " 114: 'Lee MA US',\n",
       " 115: 'Leicester MA US',\n",
       " 116: 'Lenox MA US',\n",
       " 117: 'Leominster MA US',\n",
       " 118: 'Leverett MA US',\n",
       " 119: 'Lexington MA US',\n",
       " 120: 'Lincoln MA US',\n",
       " 121: 'Littleton MA US',\n",
       " 122: 'Longmeadow MA US',\n",
       " 123: 'Lowell MA US',\n",
       " 124: 'Ludlow MA US',\n",
       " 125: 'Lunenburg MA US',\n",
       " 126: 'Lynn MA US',\n",
       " 127: 'Lynnfield MA US',\n",
       " 128: 'Malden MA US',\n",
       " 129: 'Manchester-by-the-Sea MA US',\n",
       " 130: 'Mansfield MA US',\n",
       " 131: 'Marblehead MA US',\n",
       " 132: 'Marion MA US',\n",
       " 133: 'Marlborough MA US',\n",
       " 134: 'Marshfield MA US',\n",
       " 135: 'Mashpee MA US',\n",
       " 136: 'Mattapoisett MA US',\n",
       " 137: 'Maynard MA US',\n",
       " 138: 'Medfield MA US',\n",
       " 139: 'Medford MA US',\n",
       " 140: 'Medway MA US',\n",
       " 141: 'Melrose MA US',\n",
       " 142: 'Mendon MA US',\n",
       " 143: 'Merrimac MA US',\n",
       " 144: 'Methuen Town MA US',\n",
       " 145: 'Middleborough MA US',\n",
       " 146: 'Middleton MA US',\n",
       " 147: 'Milford MA US',\n",
       " 148: 'Millbury MA US',\n",
       " 149: 'Millis MA US',\n",
       " 150: 'Millville MA US',\n",
       " 151: 'Milton MA US',\n",
       " 152: 'Montague MA US',\n",
       " 153: 'Nantucket MA US',\n",
       " 154: 'Natick MA US',\n",
       " 155: 'Needham MA US',\n",
       " 156: 'New Bedford MA US',\n",
       " 157: 'New Braintree MA US',\n",
       " 158: 'New Marlborough MA US',\n",
       " 159: 'Newbury MA US',\n",
       " 160: 'Newburyport MA US',\n",
       " 161: 'Newton MA US',\n",
       " 162: 'Norfolk MA US',\n",
       " 163: 'North Adams MA US',\n",
       " 164: 'North Andover MA US',\n",
       " 165: 'North Attleborough MA US',\n",
       " 166: 'North Reading MA US',\n",
       " 167: 'Northampton MA US',\n",
       " 168: 'Northborough MA US',\n",
       " 169: 'Northbridge MA US',\n",
       " 170: 'Norton MA US',\n",
       " 171: 'Norwell MA US',\n",
       " 172: 'Norwood MA US',\n",
       " 173: 'Oak Bluffs MA US',\n",
       " 174: 'Orange MA US',\n",
       " 175: 'Orleans MA US',\n",
       " 176: 'Oxford MA US',\n",
       " 177: 'Palmer Town MA US',\n",
       " 178: 'Paxton MA US',\n",
       " 179: 'Peabody MA US',\n",
       " 180: 'Pembroke MA US',\n",
       " 181: 'Pepperell MA US',\n",
       " 182: 'Pittsfield MA US',\n",
       " 183: 'Plainville MA US',\n",
       " 184: 'Plymouth MA US',\n",
       " 185: 'Plympton MA US',\n",
       " 186: 'Quincy MA US',\n",
       " 187: 'Randolph MA US',\n",
       " 188: 'Raynham MA US',\n",
       " 189: 'Reading MA US',\n",
       " 190: 'Rehoboth MA US',\n",
       " 191: 'Revere MA US',\n",
       " 192: 'Rockland MA US',\n",
       " 193: 'Rockport MA US',\n",
       " 194: 'Rowley MA US',\n",
       " 195: 'Russell MA US',\n",
       " 196: 'Rutland MA US',\n",
       " 197: 'Salem MA US',\n",
       " 198: 'Salisbury MA US',\n",
       " 199: 'Saugus MA US',\n",
       " 200: 'Seekonk MA US',\n",
       " 201: 'Sharon MA US',\n",
       " 202: 'Shelburne MA US',\n",
       " 203: 'Sherborn MA US',\n",
       " 204: 'Shirley MA US',\n",
       " 205: 'Shrewsbury MA US',\n",
       " 206: 'Somerset MA US',\n",
       " 207: 'Somerville MA US',\n",
       " 208: 'South Hadley MA US',\n",
       " 209: 'Southampton MA US',\n",
       " 210: 'Southborough MA US',\n",
       " 211: 'Southbridge Town MA US',\n",
       " 212: 'Spencer MA US',\n",
       " 213: 'Springfield MA US',\n",
       " 214: 'Stoneham MA US',\n",
       " 215: 'Stoughton MA US',\n",
       " 216: 'Stow MA US',\n",
       " 217: 'Sturbridge MA US',\n",
       " 218: 'Sudbury MA US',\n",
       " 219: 'Swampscott MA US',\n",
       " 220: 'Swansea MA US',\n",
       " 221: 'Taunton MA US',\n",
       " 222: 'Templeton MA US',\n",
       " 223: 'Tewksbury MA US',\n",
       " 224: 'Tisbury MA US',\n",
       " 225: 'Topsfield MA US',\n",
       " 226: 'Townsend MA US',\n",
       " 227: 'Tyngsborough MA US',\n",
       " 228: 'Tyringham MA US',\n",
       " 229: 'Wakefield MA US',\n",
       " 230: 'Walpole MA US',\n",
       " 231: 'Waltham MA US',\n",
       " 232: 'Ware MA US',\n",
       " 233: 'Wareham MA US',\n",
       " 234: 'Warren MA US',\n",
       " 235: 'Warwick MA US',\n",
       " 236: 'Watertown Town MA US',\n",
       " 237: 'Webster MA US',\n",
       " 238: 'Wellesley MA US',\n",
       " 239: 'Wenham MA US',\n",
       " 240: 'West Brookfield MA US',\n",
       " 241: 'West Newbury MA US',\n",
       " 242: 'West Springfield Town MA US',\n",
       " 243: 'West Stockbridge MA US',\n",
       " 244: 'West Tisbury MA US',\n",
       " 245: 'Westborough MA US',\n",
       " 246: 'Westfield MA US',\n",
       " 247: 'Westford MA US',\n",
       " 248: 'Westhampton MA US',\n",
       " 249: 'Westminster MA US',\n",
       " 250: 'Weston MA US',\n",
       " 251: 'Westport MA US',\n",
       " 252: 'Westwood MA US',\n",
       " 253: 'Weymouth Town MA US',\n",
       " 254: 'Whitman MA US',\n",
       " 255: 'Wilbraham MA US',\n",
       " 256: 'Williamstown MA US',\n",
       " 257: 'Wilmington MA US',\n",
       " 258: 'Winchendon MA US',\n",
       " 259: 'Winchester MA US',\n",
       " 260: 'Winthrop Town MA US',\n",
       " 261: 'Woburn MA US',\n",
       " 262: 'Worcester MA US',\n",
       " 263: 'Wrentham MA US',\n",
       " 264: 'Yarmouth MA US'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 3:\n",
    "Springfield: 213 - 5\n",
    "Lawrence: 113 - 3\n",
    "Holyoke: 104 - 2\n",
    "\n",
    "Top 3:\n",
    "Dover: 16 - 2\n",
    "Lexington: 119 - 0\n",
    "Wellesley: 238 - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep taking people from the top and bottom until its 20 on each side, then find av risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\"]\n",
    "\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble appending town names to dataframe with town codes and people counts\n",
    "\n",
    "need that so that we can then get the top twenty of each \n",
    "to then average to get rich likelihood and poor likelihood of diabetes \n",
    "yayyyy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>American house dust mite IgE Ab in Serum</th>\n",
       "      <th>Body Height</th>\n",
       "      <th>Body Mass Index</th>\n",
       "      <th>Body Weight</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Carbon Dioxide</th>\n",
       "      <th>...</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Soybean IgE Ab in Serum</th>\n",
       "      <th>Systolic Blood Pressure</th>\n",
       "      <th>Total Cholesterol</th>\n",
       "      <th>Total score [MMSE]</th>\n",
       "      <th>Triglycerides</th>\n",
       "      <th>Urea Nitrogen</th>\n",
       "      <th>Walnut IgE Ab in Serum</th>\n",
       "      <th>Wheat IgE Ab in Serum</th>\n",
       "      <th>White oak IgE Ab in Serum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0.00</td>\n",
       "      <td>724.92</td>\n",
       "      <td>74.46</td>\n",
       "      <td>244.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>0.00</td>\n",
       "      <td>666.84</td>\n",
       "      <td>131.07</td>\n",
       "      <td>364.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>489.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>771.91</td>\n",
       "      <td>30.32</td>\n",
       "      <td>96.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>536.70</td>\n",
       "      <td>91.49</td>\n",
       "      <td>292.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>352.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1647.80</td>\n",
       "      <td>347.34</td>\n",
       "      <td>943.11</td>\n",
       "      <td>93.82</td>\n",
       "      <td>247.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1514.54</td>\n",
       "      <td>183.64</td>\n",
       "      <td>525.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1226.38</td>\n",
       "      <td>229.23</td>\n",
       "      <td>301.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1507.72</td>\n",
       "      <td>350.88</td>\n",
       "      <td>677.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      race  ethnicity  gender  birthplace  \\\n",
       "892      3         10       1         137   \n",
       "1106     3          7       0         254   \n",
       "413      3          6       1          85   \n",
       "522      3          1       1          46   \n",
       "1036     2         15       1          35   \n",
       "...    ...        ...     ...         ...   \n",
       "1362     3         13       0          27   \n",
       "802      3         16       1          74   \n",
       "651      1          0       0         205   \n",
       "722      3         11       0          86   \n",
       "254      3          7       0         179   \n",
       "\n",
       "      American house dust mite IgE Ab in Serum  Body Height  Body Mass Index  \\\n",
       "892                                       0.00       724.92            74.46   \n",
       "1106                                      0.00       666.84           131.07   \n",
       "413                                       0.00         0.00             0.00   \n",
       "522                                       0.00       771.91            30.32   \n",
       "1036                                      0.00       536.70            91.49   \n",
       "...                                        ...          ...              ...   \n",
       "1362                                      0.00      1647.80           347.34   \n",
       "802                                       0.42      1514.54           183.64   \n",
       "651                                       0.00      1226.38           229.23   \n",
       "722                                       0.02      1507.72           350.88   \n",
       "254                                       0.00         0.00             0.00   \n",
       "\n",
       "      Body Weight  Calcium  Carbon Dioxide  ...  Sodium  \\\n",
       "892        244.54     0.00             0.0  ...     0.0   \n",
       "1106       364.27     0.00             0.0  ...     0.0   \n",
       "413          0.00     0.00             0.0  ...     0.0   \n",
       "522         96.65     0.00             0.0  ...     0.0   \n",
       "1036       292.85     0.00             0.0  ...     0.0   \n",
       "...           ...      ...             ...  ...     ...   \n",
       "1362       943.11    93.82           247.0  ...  1402.0   \n",
       "802        525.39     0.00             0.0  ...     0.0   \n",
       "651        301.45     0.00             0.0  ...     0.0   \n",
       "722        677.05     0.00             0.0  ...     0.0   \n",
       "254          0.00     0.00             0.0  ...     0.0   \n",
       "\n",
       "      Soybean IgE Ab in Serum  Systolic Blood Pressure  Total Cholesterol  \\\n",
       "892                      0.00                    502.0                0.0   \n",
       "1106                     0.00                    489.0              502.0   \n",
       "413                      0.00                      0.0                0.0   \n",
       "522                      0.00                   1208.0                0.0   \n",
       "1036                     0.00                    352.0              345.0   \n",
       "...                       ...                      ...                ...   \n",
       "1362                     0.00                   1246.0              566.0   \n",
       "802                      0.41                   1022.0                0.0   \n",
       "651                      0.00                   1351.0                0.0   \n",
       "722                      0.27                   1281.0                0.0   \n",
       "254                      0.00                      0.0                0.0   \n",
       "\n",
       "      Total score [MMSE]  Triglycerides  Urea Nitrogen  \\\n",
       "892                  0.0            0.0            0.0   \n",
       "1106                 0.0          355.0            0.0   \n",
       "413                  0.0            0.0            0.0   \n",
       "522                  0.0            0.0            0.0   \n",
       "1036                 0.0          262.0            0.0   \n",
       "...                  ...            ...            ...   \n",
       "1362                 0.0          324.0          132.0   \n",
       "802                  0.0            0.0            0.0   \n",
       "651                  0.0            0.0            0.0   \n",
       "722                  0.0            0.0            0.0   \n",
       "254                  0.0            0.0            0.0   \n",
       "\n",
       "      Walnut IgE Ab in Serum  Wheat IgE Ab in Serum  White oak IgE Ab in Serum  \n",
       "892                     0.00                   0.00                       0.00  \n",
       "1106                    0.00                   0.00                       0.00  \n",
       "413                     0.00                   0.00                       0.00  \n",
       "522                     0.00                   0.00                       0.00  \n",
       "1036                    0.00                   0.00                       0.00  \n",
       "...                      ...                    ...                        ...  \n",
       "1362                    0.00                   0.00                       0.00  \n",
       "802                     0.48                   0.17                       0.57  \n",
       "651                     0.00                   0.00                       0.00  \n",
       "722                     0.20                   0.06                       0.35  \n",
       "254                     0.00                   0.00                       0.00  \n",
       "\n",
       "[293 rows x 52 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df with all the information for teh rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info(town, bp_code_swapped, townCounts_df):\n",
    "    town_full = f'{town} MA US'\n",
    "    code = bp_code_swapped[town_full]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df['birthplace'] == code].empty:\n",
    "        count = townCounts_df[townCounts_df['birthplace'] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {'birthplace': town_full, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n",
    "\n",
    "townCounts_df = pd.merge(X_test, birthplace_counts, on='birthplace')\n",
    "town_info_rich = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "town_info_poor = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "\n",
    "bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "for town in richTowns:\n",
    "    \n",
    "    new_row_df = find_town_info(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "for town in poorTowns:\n",
    "    \n",
    "    new_row_df = find_town_info(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthplace</th>\n",
       "      <th>code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dover MA US</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weston MA US</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wellesley MA US</td>\n",
       "      <td>238</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lexington MA US</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sherborn MA US</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cohasset MA US</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lincoln MA US</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Carlisle MA US</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hingham MA US</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Winchester MA US</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Medfield MA US</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Concord MA US</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Needham MA US</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sudbury MA US</td>\n",
       "      <td>218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hopkinton MA US</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Boxford MA US</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Brookline MA US</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Andover MA US</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Southborough MA US</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Belmont MA US</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Acton MA US</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Marblehead MA US</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Newton MA US</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nantucket MA US</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Duxbury MA US</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Boxborough MA US</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Westwood MA US</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Natick MA US</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Longmeadow MA US</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Marion MA US</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Groton MA US</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Newbury MA US</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Andover MA US</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sharon MA US</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Arlington MA US</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Norwell MA US</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Reading MA US</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lynnfield MA US</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Marshfield MA US</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Holliston MA US</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Medway MA US</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Canton MA US</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Milton MA US</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ipswich MA US</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Littleton MA US</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Westford MA US</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>North Reading MA US</td>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             birthplace code count\n",
       "0           Dover MA US   61     1\n",
       "1          Weston MA US  250     0\n",
       "2       Wellesley MA US  238     2\n",
       "3       Lexington MA US  119     0\n",
       "4        Sherborn MA US  203     1\n",
       "5        Cohasset MA US   51     0\n",
       "6         Lincoln MA US  120     1\n",
       "7        Carlisle MA US   41     0\n",
       "8         Hingham MA US  100     3\n",
       "9      Winchester MA US  259     1\n",
       "10       Medfield MA US  138     0\n",
       "11        Concord MA US   53     0\n",
       "12        Needham MA US  155     1\n",
       "13        Sudbury MA US  218     2\n",
       "14      Hopkinton MA US  106     1\n",
       "15        Boxford MA US   30     0\n",
       "16      Brookline MA US   37     5\n",
       "17        Andover MA US    8     1\n",
       "18   Southborough MA US  210     0\n",
       "19        Belmont MA US   23     4\n",
       "20          Acton MA US    1     2\n",
       "21     Marblehead MA US  131     1\n",
       "22         Newton MA US  161     3\n",
       "23      Nantucket MA US  153     0\n",
       "24        Duxbury MA US   64     0\n",
       "25     Boxborough MA US   29     1\n",
       "26       Westwood MA US  252     0\n",
       "27         Natick MA US  154     1\n",
       "28     Longmeadow MA US  122     0\n",
       "29         Marion MA US  132     0\n",
       "30         Groton MA US   89     0\n",
       "31        Newbury MA US  159     0\n",
       "32  North Andover MA US  164     0\n",
       "33         Sharon MA US  201     1\n",
       "34      Arlington MA US    9     3\n",
       "35        Norwell MA US  171     0\n",
       "36        Reading MA US  189     0\n",
       "37      Lynnfield MA US  127     1\n",
       "38     Marshfield MA US  134     0\n",
       "39      Holliston MA US  103     0\n",
       "40         Medway MA US  140     0\n",
       "41         Canton MA US   40     1\n",
       "42         Milton MA US  151     2\n",
       "43        Ipswich MA US  110     1\n",
       "44      Littleton MA US  121     0\n",
       "45       Westford MA US  247     0\n",
       "46  North Reading MA US  166     2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "town_info_rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed with the following part to get top 50 people from each rich and poor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_towns_by_sum_pop(town_info):\n",
    "    \n",
    "    townsUsed = set()\n",
    "    peopleCount = 0\n",
    "\n",
    "    for index, row in town_info.iterrows():\n",
    "        \n",
    "        if peopleCount > 40:\n",
    "            break\n",
    "        \n",
    "        birthplace = row['birthplace']\n",
    "        count = row['count']\n",
    "        townsUsed.add(birthplace)\n",
    "        peopleCount += count\n",
    "    \n",
    "    return townsUsed, peopleCount\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich)\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amherst MA US',\n",
       " 'Athol MA US',\n",
       " 'Brockton MA US',\n",
       " 'Chelsea MA US',\n",
       " 'Chicopee MA US',\n",
       " 'Fall River MA US',\n",
       " 'Fitchburg MA US',\n",
       " 'Gardner MA US',\n",
       " 'Holyoke MA US',\n",
       " 'Lawrence MA US',\n",
       " 'Lynn MA US',\n",
       " 'Malden MA US',\n",
       " 'New Bedford MA US',\n",
       " 'Orange MA US',\n",
       " 'Springfield MA US',\n",
       " 'Worcester MA US'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poorTownsUsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_town_codes = []\n",
    "\n",
    "for town_full in richTownsUsed:\n",
    "    rich_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[127,\n",
       " 140,\n",
       " 189,\n",
       " 247,\n",
       " 164,\n",
       " 151,\n",
       " 100,\n",
       " 53,\n",
       " 138,\n",
       " 203,\n",
       " 252,\n",
       " 250,\n",
       " 131,\n",
       " 64,\n",
       " 134,\n",
       " 61,\n",
       " 155,\n",
       " 171,\n",
       " 8,\n",
       " 9,\n",
       " 41,\n",
       " 166,\n",
       " 210,\n",
       " 23,\n",
       " 159,\n",
       " 110,\n",
       " 259,\n",
       " 122,\n",
       " 238,\n",
       " 119,\n",
       " 106,\n",
       " 161,\n",
       " 37,\n",
       " 154,\n",
       " 29,\n",
       " 201,\n",
       " 121,\n",
       " 89,\n",
       " 218,\n",
       " 132,\n",
       " 1,\n",
       " 120,\n",
       " 40,\n",
       " 51,\n",
       " 30,\n",
       " 153,\n",
       " 103]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rich_town_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming subset_df is a subset of X_test based on certain conditions\n",
    "subset_df = X_test[X_test['birthplace'].isin(rich_town_codes)]\n",
    "\n",
    "# Assuming forrest is your classifier and you have already trained it\n",
    "test = forrest.predict_proba(subset_df)\n",
    "\n",
    "# Ensure that the indices of subset_df match the indices of the test array\n",
    "subset_indices = subset_df.index\n",
    "\n",
    "# Create a boolean mask for the subset indices\n",
    "mask = np.isin(subset_indices, subset_indices)\n",
    "\n",
    "# Use the boolean mask to index the test array\n",
    "prob_subset = test[mask]\n",
    "\n",
    "# Calculate the mean of the probabilities for class 1\n",
    "av_prob = np.mean(prob_subset[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37596931721156684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_town_codes = []\n",
    "\n",
    "for town_full in poorTownsUsed:\n",
    "    poor_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32149045566684353"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming subset_df is a subset of X_test based on certain conditions\n",
    "subset_df = X_test[X_test['birthplace'].isin(poor_town_codes)]\n",
    "\n",
    "# Assuming forrest is your classifier and you have already trained it\n",
    "test = forrest.predict_proba(subset_df)\n",
    "\n",
    "# Ensure that the indices of subset_df match the indices of the test array\n",
    "subset_indices = subset_df.index\n",
    "\n",
    "# Create a boolean mask for the subset indices\n",
    "mask = np.isin(subset_indices, subset_indices)\n",
    "\n",
    "# Use the boolean mask to index the test array\n",
    "prob_subset = test[mask]\n",
    "\n",
    "# Calculate the mean of the probabilities for class 1\n",
    "av_prob = np.mean(prob_subset[:, 1])\n",
    "\n",
    "av_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code=127\n",
      "risk=0.006620882778797784\n",
      "code=140\n",
      "risk=nan\n",
      "code=189\n",
      "risk=nan\n",
      "code=247\n",
      "risk=nan\n",
      "code=164\n",
      "risk=nan\n",
      "code=151\n",
      "risk=0.4992931880975984\n",
      "code=100\n",
      "risk=0.39093463489713426\n",
      "code=53\n",
      "risk=nan\n",
      "code=138\n",
      "risk=nan\n",
      "code=203\n",
      "risk=0.31231289194870165\n",
      "code=252\n",
      "risk=nan\n",
      "code=250\n",
      "risk=nan\n",
      "code=131\n",
      "risk=1.0\n",
      "code=64\n",
      "risk=nan\n",
      "code=134\n",
      "risk=nan\n",
      "code=61\n",
      "risk=0.0\n",
      "code=155\n",
      "risk=0.5484108611645043\n",
      "code=171\n",
      "risk=nan\n",
      "code=8\n",
      "risk=0.0\n",
      "code=9\n",
      "risk=0.6682738095238095\n",
      "code=41\n",
      "risk=nan\n",
      "code=166\n",
      "risk=1.0\n",
      "code=210\n",
      "risk=nan\n",
      "code=23\n",
      "risk=0.2108867926383725\n",
      "code=159\n",
      "risk=nan\n",
      "code=110\n",
      "risk=1.0\n",
      "code=259\n",
      "risk=0.0036875494454644514\n",
      "code=122\n",
      "risk=nan\n",
      "code=238\n",
      "risk=0.809994525517137\n",
      "code=119\n",
      "risk=nan\n",
      "code=106\n",
      "risk=0.006843972572439725\n",
      "code=161\n",
      "risk=0.14591749564475198\n",
      "code=37\n",
      "risk=0.20136363636363633\n",
      "code=154\n",
      "risk=0.5435226414280109\n",
      "code=29\n",
      "risk=0.0\n",
      "code=201\n",
      "risk=0.4644131866153176\n",
      "code=121\n",
      "risk=nan\n",
      "code=89\n",
      "risk=nan\n",
      "code=218\n",
      "risk=0.2891712950510133\n",
      "code=132\n",
      "risk=nan\n",
      "code=1\n",
      "risk=0.5056973319824384\n",
      "code=120\n",
      "risk=0.18231613946321665\n",
      "code=40\n",
      "risk=0.0\n",
      "code=51\n",
      "risk=nan\n",
      "code=30\n",
      "risk=nan\n",
      "code=153\n",
      "risk=nan\n",
      "code=103\n",
      "risk=nan\n",
      "rich prob:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "rich_probs = []\n",
    "\n",
    "for code in rich_town_codes:\n",
    "    risk = find_risk(code, 'birthplace', pred_prob)\n",
    "    print(f\"{code=}\")\n",
    "    print(f\"{risk=}\")\n",
    "\n",
    "    rich_probs += risk\n",
    "\n",
    "print(\"rich prob: \",np.mean(rich_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rich_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "springfield = find_risk(213, 'birthplace', pred_prob)\n",
    "lawrence = find_risk(113, 'birthplace', pred_prob)\n",
    "holyoke = find_risk(104, 'birthplace', pred_prob)\n",
    "\n",
    "dover = find_risk(16, 'birthplace', pred_prob)\n",
    "lexington = find_risk(119, 'birthplace', pred_prob)\n",
    "wellesley = find_risk(238, 'birthplace', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (X_test['birthplace'] == 238)\n",
    "prob_subset = pred_prob[indices]\n",
    "\n",
    "len(prob_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09497096492181778 \n",
      " 0.33607242670532006 \n",
      " 0.002641973391313351 \n",
      " 0.5596414541294077 \n",
      " nan \n",
      " 0.809994525517137 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(springfield, \"\\n\", lawrence, \"\\n\", holyoke, \"\\n\", dover, \"\\n\", lexington, \"\\n\", wellesley, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
