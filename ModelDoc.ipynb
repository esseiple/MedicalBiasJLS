{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "# conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patient_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate','ssn', 'address', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    \n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # MERGE DATASETS\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    \n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"curr_town\"] = le.fit_transform(merged_df[\"curr_town\"]) \n",
    "    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "    # split into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Y column to predict is diabetes\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['race', 'ethnicity', 'gender', 'birthplace', 'curr_town',\n",
       "       'American house dust mite IgE Ab in Serum', 'Body Height',\n",
       "       'Body Mass Index', 'Body Weight', 'Calcium', 'Carbon Dioxide',\n",
       "       'Cat dander IgE Ab in Serum', 'Chloride',\n",
       "       'Cladosporium herbarum IgE Ab in Serum', 'Codfish IgE Ab in Serum',\n",
       "       'Common Ragweed IgE Ab in Serum', 'Cow milk IgE Ab in Serum',\n",
       "       'Creatinine', 'DXA [T-score] Bone density', 'Diastolic Blood Pressure',\n",
       "       'Disability rating scale', 'Egg white IgE Ab in Serum',\n",
       "       'Estimated Glomerular Filtration Rate', 'FEV1/​FVC', 'Glucose',\n",
       "       'Hemoglobin A1c/Hemoglobin.total in Blood',\n",
       "       'Hemoglobin.gastrointestinal [Presence] in Stool by Immunologic method',\n",
       "       'High Density Lipoprotein Cholesterol',\n",
       "       'History of Hospitalizations+​Outpatient visits',\n",
       "       'Honey bee IgE Ab in Serum', 'Latex IgE Ab in Serum',\n",
       "       'Low Density Lipoprotein Cholesterol', 'Microalbumin Creatinine Ratio',\n",
       "       'Oral temperature', 'Peanut IgE Ab in Serum',\n",
       "       'Percentage area affected by eczema Head and Neck',\n",
       "       'Percentage area affected by eczema Lower extremitiy - bilateral',\n",
       "       'Percentage area affected by eczema Trunk',\n",
       "       'Percentage area affected by eczema Upper extremitiy - bilateral',\n",
       "       'Polyp size greatest dimension by CAP cancer protocols', 'Potassium',\n",
       "       'Quality adjusted life years', 'Shrimp IgE Ab in Serum', 'Sodium',\n",
       "       'Soybean IgE Ab in Serum', 'Systolic Blood Pressure',\n",
       "       'Total Cholesterol', 'Total score [MMSE]', 'Triglycerides',\n",
       "       'Urea Nitrogen', 'Walnut IgE Ab in Serum', 'Wheat IgE Ab in Serum',\n",
       "       'White oak IgE Ab in Serum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9050401672719269\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9153112505043837\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9016066908770772\n",
      "best score overall is:  0.9178790213124979  with model:  DTC\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# keep track of best Logistic Regression Score\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "DTCScore  = grid_search.best_score_\n",
    "bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Random Forrest Classifier    \n",
    "forrest = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "RFCScore  = grid_search.best_score_\n",
    "bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "#SVC\n",
    "SVM = SVC()\n",
    "\n",
    "# use grid search to find best gamma for SVM\n",
    "g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "print(\"best LR :\", LRScore)\n",
    "print(\"best DTC:\", DTCScore)\n",
    "print(\"best max depth: \", bestDTCDepth)\n",
    "print(\"best RFC: \", RFCScore)\n",
    "print(\"best max depth: \", bestRFCDepth)\n",
    "print(\"best SVM: \", SVMScore)\n",
    "\n",
    "max_score = 0\n",
    "max_model = \"\"\n",
    "if LRScore > max_score:\n",
    "    max_score = LRScore\n",
    "    max_model = \"LR\"\n",
    "if DTCScore > max_score:\n",
    "    max_score = DTCScore\n",
    "    max_model = \"DTC\"\n",
    "if RFCScore > max_score:\n",
    "    max_score = RFCScore\n",
    "    max_model = \"RFC\"\n",
    "if SVMScore > max_score:\n",
    "    max_score = SVMScore\n",
    "    max_model = \"SVM\"\n",
    "\n",
    "print(\"best score overall is: \", max_score, \" with model: \", max_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would compute risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asian', 1: 'black', 2: 'hispanic', 3: 'white'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define average risk score finding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Asian patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48117841854604604"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Black patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2415935787424932"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Hispanic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3264300115890997"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(2, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for white patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31777449697786764"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(3, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'F', 1: 'M'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3710968911043213"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26903321233865524"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.584878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.503842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.427310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.421840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.394510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.373603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.333730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.318237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.317365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.311595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.295063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.294423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.282515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.262089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.255613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.248071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.200508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.188358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.714286\n",
       "13            polish  0.584878\n",
       "9             german  0.503842\n",
       "12           mexican  0.427310\n",
       "1           american  0.421840\n",
       "14        portuguese  0.394510\n",
       "6            english  0.373603\n",
       "17          scottish  0.333730\n",
       "11           italian  0.318237\n",
       "5          dominican  0.317365\n",
       "15      puerto_rican  0.311595\n",
       "3   central_american  0.295063\n",
       "0            african  0.294423\n",
       "7             french  0.282515\n",
       "8    french_canadian  0.262089\n",
       "18           swedish  0.255613\n",
       "4            chinese  0.248071\n",
       "16           russian  0.200508\n",
       "10             irish  0.188358\n",
       "19       west_indian  0.000843"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep taking people from the top and bottom until its 20 on each side, then find av risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n",
    "                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n",
    "                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n",
    "\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble appending town names to dataframe with town codes and people counts\n",
    "\n",
    "need that so that we can then get the top twenty of each \n",
    "to then average to get rich likelihood and poor likelihood of diabetes \n",
    "yayyyy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df with all the information for teh rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_row(town, bp_code_swapped, townCounts_df, code_name):\n",
    "    code = bp_code_swapped[town]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df[code_name] == code].empty:\n",
    "        count = townCounts_df[townCounts_df[code_name] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {code_name: town, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_all(counts, code_name):\n",
    "    \n",
    "    townCounts_df = pd.merge(X_test, counts, on=code_name)\n",
    "    town_info_rich = pd.DataFrame(columns=[code_name, 'code', 'count'])\n",
    "    town_info_poor = pd.DataFrame(columns=[code_name, 'code', 'count'])\n",
    "\n",
    "    bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "    for town in richTowns:\n",
    "        \n",
    "        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n",
    "        town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "    for town in poorTowns:\n",
    "        \n",
    "        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n",
    "        town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n",
    "        \n",
    "    return town_info_rich, town_info_poor\n",
    "\n",
    "birthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n",
    "\n",
    "town_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthplace</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>261</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>262</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>264</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     birthplace  count\n",
       "0             1      2\n",
       "1             3      1\n",
       "2             4      5\n",
       "3             7      1\n",
       "4             8      1\n",
       "..          ...    ...\n",
       "139         259      1\n",
       "140         260      2\n",
       "141         261      3\n",
       "142         262      9\n",
       "143         264      4\n",
       "\n",
       "[144 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birthplace_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed with the following part to get top 65 people from each rich and poor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_towns_by_sum_pop(town_info, code_name):\n",
    "    \n",
    "    townsUsed = set()\n",
    "    peopleCount = 0\n",
    "\n",
    "    for index, row in town_info.iterrows():\n",
    "        \n",
    "        if peopleCount > 65:\n",
    "            break\n",
    "        \n",
    "        name = row[code_name]\n",
    "        count = row['count']\n",
    "        townsUsed.add(name)\n",
    "        peopleCount += count\n",
    "    \n",
    "    return townsUsed, peopleCount\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33603541093147665"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_av_prob_bp(townsUsed, code_name, bp_code):\n",
    "    \n",
    "    town_codes = []\n",
    "    bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "\n",
    "    for town_full in townsUsed:\n",
    "        town_codes.append(bp_code_swapped[town_full])\n",
    "        \n",
    "    indices = X_test[code_name].isin(town_codes)\n",
    "    prob_subset = pred_prob[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "    return av_prob\n",
    "\n",
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\n",
    "av_rich_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3231101542941584"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n",
    "av_poor_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really annoying result..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating Process for Current Town of Residence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure what this code does lol... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_town_info_sep_df(town, curr_code_swapped, townCounts_df):\n",
    "#     code = curr_code_swapped[town]\n",
    "    \n",
    "#     if not townCounts_df[townCounts_df['curr_town'] == code].empty:\n",
    "#         count = townCounts_df[townCounts_df['curr_town'] == code]['count'].values[0]\n",
    "#     else:\n",
    "#         count = 0\n",
    "    \n",
    "#     new_row = {'curr_town': town, 'code': code, 'count': count}\n",
    "    \n",
    "#     new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "#     return new_row_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the information for rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\n",
    "\n",
    "town_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again get the towns with total for 65 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24287085165813543"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\n",
    "av_rich_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3231101542941584"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\n",
    "\n",
    "av_poor_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People from rich towns have lower rates of diabetes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
