{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "# conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patient_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate', 'address','ssn', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    \n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # MERGE DATASETS\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    \n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"curr_town\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    # split into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Y column to predict is diabetes\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9076079380800411\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9204467921206119\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9127361432082463\n",
      "best score overall is:  0.9204467921206119  with model:  RFC\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# keep track of best Logistic Regression Score\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "DTCScore  = grid_search.best_score_\n",
    "bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Random Forrest Classifier    \n",
    "forrest = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "RFCScore  = grid_search.best_score_\n",
    "bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "#SVC\n",
    "SVM = SVC()\n",
    "\n",
    "# use grid search to find best gamma for SVM\n",
    "g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "print(\"best LR :\", LRScore)\n",
    "print(\"best DTC:\", DTCScore)\n",
    "print(\"best max depth: \", bestDTCDepth)\n",
    "print(\"best RFC: \", RFCScore)\n",
    "print(\"best max depth: \", bestRFCDepth)\n",
    "print(\"best SVM: \", SVMScore)\n",
    "\n",
    "max_score = 0\n",
    "max_model = \"\"\n",
    "if LRScore > max_score:\n",
    "    max_score = LRScore\n",
    "    max_model = \"LR\"\n",
    "if DTCScore > max_score:\n",
    "    max_score = DTCScore\n",
    "    max_model = \"DTC\"\n",
    "if RFCScore > max_score:\n",
    "    max_score = RFCScore\n",
    "    max_model = \"RFC\"\n",
    "if SVMScore > max_score:\n",
    "    max_score = SVMScore\n",
    "    max_model = \"SVM\"\n",
    "\n",
    "print(\"best score overall is: \", max_score, \" with model: \", max_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would compute risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asian', 1: 'black', 2: 'hispanic', 3: 'white'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define average risk score finding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Asian patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47878674410471816"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Black patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23158344065491251"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Hispanic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3251533929193315"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(2, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for white patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31531738065421405"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(3, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'F', 1: 'M'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36737260894307217"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26664582651375324"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.579510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.493665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.420759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.395919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.370546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.334219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.318602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.308682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.297509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.293891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.287088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.280231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.259318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.248659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.243288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.200869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.185791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.001448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.714286\n",
       "13            polish  0.579510\n",
       "9             german  0.493665\n",
       "12           mexican  0.433000\n",
       "1           american  0.420759\n",
       "14        portuguese  0.395919\n",
       "6            english  0.370546\n",
       "17          scottish  0.334219\n",
       "11           italian  0.318602\n",
       "15      puerto_rican  0.308682\n",
       "5          dominican  0.297509\n",
       "3   central_american  0.293891\n",
       "0            african  0.287088\n",
       "7             french  0.280231\n",
       "8    french_canadian  0.259318\n",
       "18           swedish  0.248659\n",
       "4            chinese  0.243288\n",
       "16           russian  0.200869\n",
       "10             irish  0.185791\n",
       "19       west_indian  0.001448"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep taking people from the top and bottom until its 20 on each side, then find av risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n",
    "                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n",
    "                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n",
    "\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble appending town names to dataframe with town codes and people counts\n",
    "\n",
    "need that so that we can then get the top twenty of each \n",
    "to then average to get rich likelihood and poor likelihood of diabetes \n",
    "yayyyy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df with all the information for teh rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_bp(town, bp_code_swapped, townCounts_df):\n",
    "    code = bp_code_swapped[town]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df['birthplace'] == code].empty:\n",
    "        count = townCounts_df[townCounts_df['birthplace'] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {'birthplace': town, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n",
    "\n",
    "townCounts_df = pd.merge(X_test, birthplace_counts, on='birthplace')\n",
    "town_info_rich = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "town_info_poor = pd.DataFrame(columns=['birthplace', 'code', 'count'])\n",
    "\n",
    "bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "for town in richTowns:\n",
    "    \n",
    "    new_row_df = find_town_info_bp(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "for town in poorTowns:\n",
    "    \n",
    "    new_row_df = find_town_info_bp(town, bp_code_swapped, townCounts_df)\n",
    "    town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed with the following part to get top 65 people from each rich and poor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_towns_by_sum_pop_bp(town_info):\n",
    "    \n",
    "    townsUsed = set()\n",
    "    peopleCount = 0\n",
    "\n",
    "    for index, row in town_info.iterrows():\n",
    "        \n",
    "        if peopleCount > 65:\n",
    "            break\n",
    "        \n",
    "        birthplace = row['birthplace']\n",
    "        count = row['count']\n",
    "        townsUsed.add(birthplace)\n",
    "        peopleCount += count\n",
    "    \n",
    "    return townsUsed, peopleCount\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_town_codes = []\n",
    "\n",
    "for town_full in richTownsUsed:\n",
    "    rich_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3296620470605214"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = X_test['birthplace'].isin(rich_town_codes)\n",
    "prob_subset = pred_prob[indices]\n",
    "av_rich_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "av_rich_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_town_codes = []\n",
    "\n",
    "for town_full in poorTownsUsed:\n",
    "    poor_town_codes.append(bp_code_swapped[town_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3207533354736699"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = X_test['birthplace'].isin(poor_town_codes)\n",
    "prob_subset = pred_prob[indices]\n",
    "av_poor_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "av_poor_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really annoying result..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating Process for Current Town of Residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_curr(town, curr_code_swapped, townCounts_df):\n",
    "    code = curr_code_swapped[town]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df['curr_town'] == code].empty:\n",
    "        count = townCounts_df[townCounts_df['curr_town'] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {'curr_town': town, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Dover'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m curr_code_swapped \u001b[38;5;241m=\u001b[39m {value: key \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m curr_code\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m town \u001b[38;5;129;01min\u001b[39;00m richTowns:\n\u001b[0;32m---> 11\u001b[0m     new_row_df \u001b[38;5;241m=\u001b[39m \u001b[43mfind_town_info_curr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_code_swapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtownCounts_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     town_info_rich \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([town_info_rich, new_row_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m town \u001b[38;5;129;01min\u001b[39;00m poorTowns:\n",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m, in \u001b[0;36mfind_town_info_curr\u001b[0;34m(town, curr_code_swapped, townCounts_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_town_info_curr\u001b[39m(town, curr_code_swapped, townCounts_df):\n\u001b[0;32m----> 2\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_code_swapped\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtown\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m townCounts_df[townCounts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurr_town\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m code]\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      5\u001b[0m         count \u001b[38;5;241m=\u001b[39m townCounts_df[townCounts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurr_town\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m code][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Dover'"
     ]
    }
   ],
   "source": [
    "curr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\n",
    "\n",
    "townCounts_df = pd.merge(X_test, curr_counts, on='curr_town')\n",
    "town_info_rich = pd.DataFrame(columns=['curr_town', 'code', 'count'])\n",
    "town_info_poor = pd.DataFrame(columns=['curr_town', 'code', 'count'])\n",
    "\n",
    "curr_code_swapped = {value: key for key, value in curr_code.items()}\n",
    "\n",
    "for town in richTowns:\n",
    "    \n",
    "    new_row_df = find_town_info_curr(town, curr_code_swapped, townCounts_df)\n",
    "    town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "for town in poorTowns:\n",
    "    \n",
    "    new_row_df = find_town_info_curr(town, curr_code_swapped, townCounts_df)\n",
    "    town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
