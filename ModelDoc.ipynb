{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Societal Inequity's Effect on (Model-Perceived) Health Outcomes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING & ANALYSIS DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project, we aim to explore the relationship between diseases and social factors such as sex, race, and town, and how these may reflect societal and enviornmental inequities. Our approach is to identify the most accurate predictive model for our dataset, then use this model to generate risk likelihood scores and evaluate the relationship between different diseases and characteristics indicative of societal inequalities. We will then analyze the implications of these risk factors for inequitable, identity-based risk factors in health outcomes and complications. Our project consists of three documents, one in which we clean our original data, one in which we explore this data visually, and the final one, this one, in which we build and explore our models.\n",
    "\n",
    "Taking the general trends we witness in our data visualization document, we carried out the second half of our project; building a model that predicts risk scores. Comparing the risk scores, we wanted to see whether trends emerged in terms of socioeconomic status (which we measure by the proxy of town of residence), race, gender, and ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "conditions_pregnancy = pd.read_csv('conditions_pregnancy.csv')\n",
    "conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patient_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: All of our datasets are grouped by related dieases (for example diabetes and comorbitidies such as diabetic retinopathy), for the rest of the post, when we say \"diabetes\" or \"pregnancy complications,\" we are talking about diabetes and all present comorbidites, or a grouping of pregnancy complications such as pre/ante eclampsia and misscarriage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Modeling & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prep our data for modelling we label encoded each of the qualitative variables (keeping track so we could decode them again later). We created a function in order to do this easily multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# our data-prepping function for modeling\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "\n",
    "    # make patients column match others for merging, drop unnecessary information and NA vals\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate','ssn', 'address', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # merge datasets (patient info and corresponding conditions)\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    # create y\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "\n",
    "    # label encode all quantitative vars\n",
    "    \n",
    "    # race\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "    # ethnicity\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    # gender\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    # birthplace\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "    # current town of residence\n",
    "    merged_df[\"curr_town\"] = le.fit_transform(merged_df[\"curr_town\"]) \n",
    "    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "    # split data into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    # return split x, y, and all of the code tracking dicts\n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using above function to create diabetes test and train set\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finding optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created a function we could reuse that identifies the best performing model on our data from the options random forest, SVC, logistic regression, and decision trees. The best model is what we use to predict the probability that each person has a certain disease (for our purposes, their risk score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9050401672719269\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9153112505043837\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9016066908770772\n",
      "best score overall is:  0.9178790213124979  with model:  DTC\n"
     ]
    }
   ],
   "source": [
    "# our model-finding function\n",
    "def train_model(X_train, y_train):\n",
    "\n",
    "    #LogisticRegression\n",
    "    LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "    LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    #DecisionTreeClassifier\n",
    "    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "    tree = DecisionTreeClassifier()\n",
    "    grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    DTCScore  = grid_search.best_score_\n",
    "    bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "    # Random Forrest Classifier    \n",
    "    forrest = RandomForestClassifier(random_state=0)\n",
    "    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    RFCScore  = grid_search.best_score_\n",
    "    bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "    #SVC\n",
    "    SVM = SVC()\n",
    "\n",
    "    # use grid search to find best gamma for SVM\n",
    "    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "    grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "    print(\"best LR :\", LRScore)\n",
    "    print(\"best DTC:\", DTCScore)\n",
    "    print(\"best max depth: \", bestDTCDepth)\n",
    "    print(\"best RFC: \", RFCScore)\n",
    "    print(\"best max depth: \", bestRFCDepth)\n",
    "    print(\"best SVM: \", SVMScore)\n",
    "\n",
    "    # store the scores of each model\n",
    "    max_score = 0\n",
    "    max_model = \"\"\n",
    "    if LRScore > max_score:\n",
    "        max_score = LRScore\n",
    "        max_model = \"LR\"\n",
    "    if DTCScore > max_score:\n",
    "        max_score = DTCScore\n",
    "        max_model = \"DTC\"\n",
    "    if RFCScore > max_score:\n",
    "        max_score = RFCScore\n",
    "        max_model = \"RFC\"\n",
    "    if SVMScore > max_score:\n",
    "        max_score = SVMScore\n",
    "        max_model = \"SVM\"\n",
    "\n",
    "    print(\"best score overall is: \", max_score, \" with model: \", max_model)\n",
    "    \n",
    "# run model finding function on our diabetes data\n",
    "train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our function should that the decision tree classifier is the best model possible, with an accuracy of 91.78%. Our accuracies tend generally lower considering the limited information we allowed the model to have, as we really wanted to see what the model would do when it predicted on identity factors such as race, ethnicity, and birthplace, and not how it would predict given information on the specific procedures and allergies a patient had."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Risk Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found (random forest). HELLO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease we created a risk finding function that can be used across factors and disease probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    # finds the corresponding subset of our probability data\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    # finds the average of this subset\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare Across Race, Gender, Ethnicity\n",
    "Next, we find the average risk score for different demographic characteristics: Race, Gender, and Ethnicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asian</td>\n",
       "      <td>0.478016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hispanic</td>\n",
       "      <td>0.325443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>0.314769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>0.231741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race      risk\n",
       "0     asian  0.478016\n",
       "2  hispanic  0.325443\n",
       "3     white  0.314769\n",
       "1     black  0.231741"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetesRaceRisk = []\n",
    "\n",
    "# find risk for each race (after finding on their code from the label encoder)\n",
    "for code, race in race_code.items():\n",
    "    avRisk = find_risk(code, 'race', pred_prob)\n",
    "    newRow = {'race': race, 'risk': avRisk}\n",
    "    diabetesRaceRisk.append(newRow)\n",
    "\n",
    "# print summary table\n",
    "diabetesRaceRisk = pd.DataFrame(diabetesRaceRisk)\n",
    "diabetesRaceRisk = diabetesRaceRisk.sort_values(by='risk', ascending=False)\n",
    "diabetesRaceRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model tells us that the most susceptible group to diabetes is Asian, then Hispanic and White, with Black being the least susceptible. These results were interesting in that they do indeed indicate that there may be a difference according to race, and made us think of how we could explore demographic information about Massachussetts (where our data is \"from\"), to understand whether these trends are reflective of larger trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.367367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.265909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender      risk\n",
       "0      F  0.367367\n",
       "1      M  0.265909"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetesGenderRisk = []\n",
    "\n",
    "for code, gender in gen_code.items():\n",
    "    avRisk = find_risk(code, 'gender', pred_prob)\n",
    "    newRow = {'gender': gender, 'risk': avRisk}\n",
    "    diabetesGenderRisk.append(newRow)\n",
    "\n",
    "diabetesGenderRisk = pd.DataFrame(diabetesGenderRisk)\n",
    "diabetesGenderRisk = diabetesGenderRisk.sort_values(by='risk', ascending=False)\n",
    "diabetesGenderRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model tells us that women are slightly more likely to experience diabetes (or comorbidities) than men, which is in line with medical research we've seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.715515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.580198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.493073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.431227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.422170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.398262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.369399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.336727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.315302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.309661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.294521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.293388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.288309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.274024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.264564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.247927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.240516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.204513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.186410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.715515\n",
       "13            polish  0.580198\n",
       "9             german  0.493073\n",
       "12           mexican  0.431227\n",
       "1           american  0.422170\n",
       "14        portuguese  0.398262\n",
       "6            english  0.369399\n",
       "17          scottish  0.336727\n",
       "11           italian  0.315302\n",
       "15      puerto_rican  0.309661\n",
       "5          dominican  0.294521\n",
       "3   central_american  0.293388\n",
       "0            african  0.288309\n",
       "7             french  0.274024\n",
       "8    french_canadian  0.264564\n",
       "18           swedish  0.247927\n",
       "4            chinese  0.240516\n",
       "16           russian  0.204513\n",
       "10             irish  0.186410\n",
       "19       west_indian  0.004541"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n",
    "av_risk_eth_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table gives us lots of information about risk by ethnicity, most interestingly perhaps, it agrees with our race finding that Asian people are more likely to experience diabetes, in that our most at risk ethnicity was Asian Indian. However, Chinese and West Indian, the ttwo other Asian ethnicities in the datasest are at the bottom of the risk hierarchy, which made us consider that the risk of Asian Indian people specifically, and alone, was what was driving our other race findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compare Across Wealthier & Poorer TOwns of Residence/Birthplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare outcomes across towns of varying socioeconomic status, we compiled a list of the richest and poorest towns present in our dataset (using Census data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# richest towns in Mass\n",
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n",
    "                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n",
    "                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n",
    "\n",
    "# poorest towns in Mass\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df with all the information for the rich and poor towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_row(town, bp_code_swapped, townCounts_df, code_name):\n",
    "    code = bp_code_swapped[town]\n",
    "    \n",
    "    if not townCounts_df[townCounts_df[code_name] == code].empty:\n",
    "        count = townCounts_df[townCounts_df[code_name] == code]['count'].values[0]\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    new_row = {code_name: town, 'code': code, 'count': count}\n",
    "    \n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    \n",
    "    return new_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_town_info_all(counts, code_name):\n",
    "    \n",
    "    townCounts_df = pd.merge(X_test, counts, on=code_name)\n",
    "    town_info_rich = pd.DataFrame(columns=[code_name, 'code', 'count'])\n",
    "    town_info_poor = pd.DataFrame(columns=[code_name, 'code', 'count'])\n",
    "\n",
    "    bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "    for town in richTowns:\n",
    "        \n",
    "        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n",
    "        town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n",
    "\n",
    "    for town in poorTowns:\n",
    "        \n",
    "        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n",
    "        town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n",
    "        \n",
    "    return town_info_rich, town_info_poor\n",
    "\n",
    "birthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n",
    "\n",
    "town_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the following code to get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_towns_by_sum_pop(town_info, code_name):\n",
    "    \n",
    "    townsUsed = set()\n",
    "    peopleCount = 0\n",
    "\n",
    "    for index, row in town_info.iterrows():\n",
    "        \n",
    "        if peopleCount > 65:\n",
    "            break\n",
    "        \n",
    "        name = row[code_name]\n",
    "        count = row['count']\n",
    "        townsUsed.add(name)\n",
    "        peopleCount += count\n",
    "    \n",
    "    return townsUsed, peopleCount\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birthplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_prob_bp(townsUsed, code_name, bp_code):\n",
    "    \n",
    "    town_codes = []\n",
    "    bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "\n",
    "\n",
    "    for town_full in townsUsed:\n",
    "        town_codes.append(bp_code_swapped[town_full])\n",
    "        \n",
    "    indices = X_test[code_name].isin(town_codes)\n",
    "    prob_subset = pred_prob[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "\n",
    "    return av_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.3303070301328279 av_poor_prob:  0.32079424012922164\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that there is not much difference in the average risk of diabetes when comparing poor and rich birthplace towns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Town of Residence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the information for rich and poor towns. Then get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\n",
    "town_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\n",
    "\n",
    "richTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\n",
    "poorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.24047650107032187 av_poor_prob:  0.277944129347204\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comparison, we find that people currently residing in rich towns have lower rates of diabetes than those residing in poorer towns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregnancy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preg_descriptions = ['PATIENT', 'Miscarriage in first trimester_CONDITIONS',\n",
    "                        'Miscarriage in second trimester_CONDITIONS',\n",
    "                        'Complication occuring during pregnancy_CONDITIONS',\n",
    "                        'Preeclampsia_CONDITIONS', 'Antepartum eclampsia_CONDITIONS',\n",
    "                        'Tubal pregnancy_CONDITIONS', 'Congenital uterine anomaly_CONDITIONS',\n",
    "                        'Blighted ovum_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(patients, conditions_pregnancy, preg_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9538094714060378\n",
      "best DTC: 0.9632185172957705\n",
      "best max depth:  {'max_depth': 1}\n",
      "best RFC:  0.9632185172957705\n",
      "best max depth:  {'max_depth': 1}\n",
      "best SVM:  0.9632185172957705\n",
      "best score overall is:  0.9632185172957705  with model:  DTC\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Average Risk scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(max_depth=5)\n",
    "DTC.fit(X_train, y_train)\n",
    "pred_prob = DTC.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>0.075689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hispanic</td>\n",
       "      <td>0.061280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>0.031715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asian</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race      risk\n",
       "1     black  0.075689\n",
       "2  hispanic  0.061280\n",
       "3     white  0.031715\n",
       "0     asian  0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregRaceRisk = []\n",
    "\n",
    "for code, race in race_code.items():\n",
    "    avRisk = find_risk(code, 'race', pred_prob)\n",
    "    newRow = {'race': race, 'risk': avRisk}\n",
    "    pregRaceRisk.append(newRow)\n",
    "\n",
    "pregRaceRisk = pd.DataFrame(pregRaceRisk)\n",
    "pregRaceRisk = pregRaceRisk.sort_values(by='risk', ascending=False)\n",
    "pregRaceRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.078311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender      risk\n",
       "0      F  0.078311\n",
       "1      M  0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregGenderRisk = []\n",
    "\n",
    "for code, gender in gen_code.items():\n",
    "    avRisk = find_risk(code, 'gender', pred_prob)\n",
    "    newRow = {'gender': gender, 'risk': avRisk}\n",
    "    pregGenderRisk.append(newRow)\n",
    "\n",
    "pregGenderRisk = pd.DataFrame(pregGenderRisk)\n",
    "pregGenderRisk = pregGenderRisk.sort_values(by='risk', ascending=False)\n",
    "pregGenderRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result may seem a bit redundant or silly, it makes sense as generally men do not get pregnant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.159329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.147799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.119497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.073537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.047799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.039832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.039832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.038756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.038547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.034142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.028355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.026555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.021727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.017071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.014937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "17          scottish  0.159329\n",
       "5          dominican  0.147799\n",
       "19       west_indian  0.119497\n",
       "15      puerto_rican  0.073537\n",
       "7             french  0.047799\n",
       "12           mexican  0.039832\n",
       "8    french_canadian  0.039832\n",
       "11           italian  0.038756\n",
       "6            english  0.038547\n",
       "3   central_american  0.034142\n",
       "10             irish  0.028355\n",
       "13            polish  0.026555\n",
       "1           american  0.021727\n",
       "14        portuguese  0.017071\n",
       "9             german  0.014937\n",
       "0            african  0.000000\n",
       "2       asian_indian  0.000000\n",
       "4            chinese  0.000000\n",
       "16           russian  0.000000\n",
       "18           swedish  0.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birthplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.02573778422835027 av_poor_prob:  0.04847605224963716\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Town of Residence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.03309143686502177 av_poor_prob:  0.018384131591678763\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for birthplace towns, those in less-wealthy areas have a higher risk for pregnancy complications. However in the risk scores for current town of residence, people in wealthier areas have a higher risk for pregnancy complications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9777557683137083\n",
      "best DTC: 0.9854554124940391\n",
      "best max depth:  {'max_depth': 1}\n",
      "best RFC:  0.9803198708778108\n",
      "best max depth:  {'max_depth': 7}\n",
      "best SVM:  0.9546641722607386\n",
      "best score overall is:  0.9854554124940391  with model:  DTC\n"
     ]
    }
   ],
   "source": [
    "cancer_descriptions = ['PATIENT', 'Non-small cell lung cancer (disorder)_CONDITIONS',\n",
    "                        'Non-small cell carcinoma of lung  TNM stage 4 (disorder)_CONDITIONS',\n",
    "                        'Primary small cell malignant neoplasm of lung  TNM stage 4 (disorder)_CONDITIONS',\n",
    "                        'Non-small cell carcinoma of lung  TNM stage 2 (disorder)_CONDITIONS',\n",
    "                        'Non-small cell lung cancer (disorder)_CONDITIONS','Suspected lung cancer (situation)_CONDITIONS',\n",
    "                        'Malignant tumor of colon_CONDITIONS','Overlapping malignant neoplasm of colon_CONDITIONS']\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(patients, conditions_cancer, cancer_descriptions, observations)\n",
    "\n",
    "#getting rid of few NaN values\n",
    "X_train.fillna(0.0, inplace=True)\n",
    "#train the model\n",
    "train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we find that the model with the best score is DTC, The Decision Tree Classifier, with about 98% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DTC \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m DTC\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 3\u001b[0m pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mDTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/tree/_classes.py:922\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities of the input samples X.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03mThe predicted class probability is the fraction of samples of the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    classes corresponds to that in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    921\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 922\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/tree/_classes.py:392\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[0;32m--> 392\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    394\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[1;32m    395\u001b[0m     ):\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier(max_depth=5)\n",
    "DTC.fit(X_train, y_train)\n",
    "pred_prob = DTC.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asian</td>\n",
       "      <td>0.081074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hispanic</td>\n",
       "      <td>0.061999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>0.042912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>0.035829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race      risk\n",
       "0     asian  0.081074\n",
       "2  hispanic  0.061999\n",
       "1     black  0.042912\n",
       "3     white  0.035829"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancerRaceRisk = []\n",
    "\n",
    "for code, race in race_code.items():\n",
    "    avRisk = find_risk(code, 'race', pred_prob)\n",
    "    newRow = {'race': race, 'risk': avRisk}\n",
    "    cancerRaceRisk.append(newRow)\n",
    "\n",
    "cancerRaceRisk = pd.DataFrame(cancerRaceRisk)\n",
    "cancerRaceRisk = cancerRaceRisk.sort_values(by='risk', ascending=False)\n",
    "cancerRaceRisk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that Asian people are the most likely to have, or get diagnosed with, cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.058980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.024786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender      risk\n",
       "1      M  0.058980\n",
       "0      F  0.024786"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancerGenderRisk = []\n",
    "\n",
    "for code, gender in gen_code.items():\n",
    "    avRisk = find_risk(code, 'gender', pred_prob)\n",
    "    newRow = {'gender': gender, 'risk': avRisk}\n",
    "    cancerGenderRisk.append(newRow)\n",
    "\n",
    "cancerGenderRisk = pd.DataFrame(cancerGenderRisk)\n",
    "cancerGenderRisk = cancerGenderRisk.sort_values(by='risk', ascending=False)\n",
    "cancerGenderRisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable result that men are over twice as likely to get/get diagnosed with cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethnicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.162149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.093056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.087311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.076344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.068307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.019292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.018803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.017460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.009003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.008721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "4            chinese  0.162149\n",
       "5          dominican  0.100000\n",
       "9             german  0.093056\n",
       "15      puerto_rican  0.087311\n",
       "6            english  0.076344\n",
       "11           italian  0.068307\n",
       "1           american  0.022222\n",
       "3   central_american  0.019292\n",
       "0            african  0.018803\n",
       "14        portuguese  0.017460\n",
       "7             french  0.009003\n",
       "10             irish  0.008721\n",
       "16           russian  0.005128\n",
       "12           mexican  0.002137\n",
       "8    french_canadian  0.002137\n",
       "13            polish  0.000000\n",
       "2       asian_indian  0.000000\n",
       "17          scottish  0.000000\n",
       "18           swedish  0.000000\n",
       "19       west_indian  0.000000"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n",
    "\n",
    "av_risk_eth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birthplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.011676528599605522 av_poor_prob:  0.058422090729783036\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Town of Residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "av_rich_prob:  0.03700197238658777 av_poor_prob:  0.06381766381766382\n"
     ]
    }
   ],
   "source": [
    "av_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\n",
    "av_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n",
    "\n",
    "print(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for both birthplace towns and town of current residence, people in rich towns are half as likely to get diagnosed with cancer as opposed to people from poorer towns. This suggests environmental and systemic issues that contribute to the poorer health of those from less-wealthy areas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
