{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6v/npbkg9_919s0m2j5qd702gf00000gn/T/ipykernel_22985/1372183016.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_diabetes = pd.read_csv('conditions_diabetes.csv')\n",
    "# conditions_cancer = pd.read_csv('conditions_cancer.csv')\n",
    "observations = pd.read_csv('observations_pivot.csv')\n",
    "patients = pd.read_csv('patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "def prep_data(patients, conditions, illness_descriptions, observations):\n",
    "    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n",
    "    patients = patients.drop(columns=['birthdate', 'marital','deathdate', 'address','ssn', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n",
    "    \n",
    "    patients = patients.dropna()\n",
    "    conditions = conditions.dropna()\n",
    "\n",
    "    # MERGE DATASETS\n",
    "    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n",
    "    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n",
    "\n",
    "    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n",
    "    \n",
    "    merged_df = merged_df.drop(columns=illness_descriptions)\n",
    "    merged_df[\"race\"] = le.fit_transform(merged_df[\"race\"]) \n",
    "    race_code = {code: race for code, race in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    merged_df[\"ethnicity\"] = le.fit_transform(merged_df[\"ethnicity\"])\n",
    "    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"gender\"] = le.fit_transform(merged_df[\"gender\"])  \n",
    "    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n",
    "\n",
    "    merged_df[\"birthplace\"] = le.fit_transform(merged_df[\"birthplace\"]) \n",
    "    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n",
    "\n",
    "\n",
    "    # split into test and train\n",
    "    train, test = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Y column to predict is diabetes\n",
    "    X_train = train.drop(columns=['y'])\n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test.drop(columns=['y'])\n",
    "    y_test = test['y']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code\n",
    "\n",
    "illness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n",
    "                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n",
    "X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code = prep_data(patients, conditions_diabetes, illness_descriptions, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best LR : 0.9067532372253403\n",
      "best DTC: 0.9178790213124979\n",
      "best max depth:  {'max_depth': 3}\n",
      "best RFC:  0.9195847547778879\n",
      "best max depth:  {'max_depth': 5}\n",
      "best SVM:  0.9144492131616596\n",
      "best score overall is:  0.9195847547778879  with model:  RFC\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "LR = LogisticRegression(max_iter=10000000000000000000)\n",
    "LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n",
    "\n",
    "# keep track of best Logistic Regression Score\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "DTCScore  = grid_search.best_score_\n",
    "bestDTCDepth = grid_search.best_params_\n",
    "\n",
    "\n",
    "# Random Forrest Classifier    \n",
    "forrest = RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(forrest, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "RFCScore  = grid_search.best_score_\n",
    "bestRFCDepth = grid_search.best_params_\n",
    "\n",
    "#SVC\n",
    "SVM = SVC()\n",
    "\n",
    "# use grid search to find best gamma for SVM\n",
    "g = {'gamma': 10.0 ** np.arange(-5, 5) }\n",
    "grid_search = GridSearchCV(SVM, g, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "SVMScore  = grid_search.best_score_   \n",
    "\n",
    "\n",
    "print(\"best LR :\", LRScore)\n",
    "print(\"best DTC:\", DTCScore)\n",
    "print(\"best max depth: \", bestDTCDepth)\n",
    "print(\"best RFC: \", RFCScore)\n",
    "print(\"best max depth: \", bestRFCDepth)\n",
    "print(\"best SVM: \", SVMScore)\n",
    "\n",
    "max_score = 0\n",
    "max_model = \"\"\n",
    "if LRScore > max_score:\n",
    "    max_score = LRScore\n",
    "    max_model = \"LR\"\n",
    "if DTCScore > max_score:\n",
    "    max_score = DTCScore\n",
    "    max_model = \"DTC\"\n",
    "if RFCScore > max_score:\n",
    "    max_score = RFCScore\n",
    "    max_model = \"RFC\"\n",
    "if SVMScore > max_score:\n",
    "    max_score = SVMScore\n",
    "    max_model = \"SVM\"\n",
    "\n",
    "print(\"best score overall is: \", max_score, \" with model: \", max_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next would compute risk scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'asian', 1: 'black', 2: 'hispanic', 3: 'white'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict probabilities for all our entries using the best model we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forrest = RandomForestClassifier(max_depth=5)\n",
    "forrest.fit(X_train, y_train)\n",
    "pred_prob = forrest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define average risk score finding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk(code, col, probs):\n",
    "    indices = (X_test[col] == code)\n",
    "    prob_subset = probs[indices]\n",
    "    av_prob = np.mean(prob_subset[:, 1]) \n",
    "    return av_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Asian patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48188497404065167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Black patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24022361621907298"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for Hispanic patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32889915821708876"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(2, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk score for white patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3159515925650529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(3, 'race', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute av. risk for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'F', 1: 'M'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37010866159600764"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(0, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26785288885093805"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_risk(1, 'gender', pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'african',\n",
       " 1: 'american',\n",
       " 2: 'asian_indian',\n",
       " 3: 'central_american',\n",
       " 4: 'chinese',\n",
       " 5: 'dominican',\n",
       " 6: 'english',\n",
       " 7: 'french',\n",
       " 8: 'french_canadian',\n",
       " 9: 'german',\n",
       " 10: 'irish',\n",
       " 11: 'italian',\n",
       " 12: 'mexican',\n",
       " 13: 'polish',\n",
       " 14: 'portuguese',\n",
       " 15: 'puerto_rican',\n",
       " 16: 'russian',\n",
       " 17: 'scottish',\n",
       " 18: 'swedish',\n",
       " 19: 'west_indian'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asian_indian</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>polish</td>\n",
       "      <td>0.581169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>german</td>\n",
       "      <td>0.495950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mexican</td>\n",
       "      <td>0.435955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.420880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>0.395776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.373404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scottish</td>\n",
       "      <td>0.334445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>italian</td>\n",
       "      <td>0.319817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dominican</td>\n",
       "      <td>0.314767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puerto_rican</td>\n",
       "      <td>0.313822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>central_american</td>\n",
       "      <td>0.293136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>african</td>\n",
       "      <td>0.292962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>0.281670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french_canadian</td>\n",
       "      <td>0.259378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chinese</td>\n",
       "      <td>0.249484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>swedish</td>\n",
       "      <td>0.237179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>russian</td>\n",
       "      <td>0.200952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>irish</td>\n",
       "      <td>0.185516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>west_indian</td>\n",
       "      <td>0.001717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 eth      risk\n",
       "2       asian_indian  0.714286\n",
       "13            polish  0.581169\n",
       "9             german  0.495950\n",
       "12           mexican  0.435955\n",
       "1           american  0.420880\n",
       "14        portuguese  0.395776\n",
       "6            english  0.373404\n",
       "17          scottish  0.334445\n",
       "11           italian  0.319817\n",
       "5          dominican  0.314767\n",
       "15      puerto_rican  0.313822\n",
       "3   central_american  0.293136\n",
       "0            african  0.292962\n",
       "7             french  0.281670\n",
       "8    french_canadian  0.259378\n",
       "4            chinese  0.249484\n",
       "18           swedish  0.237179\n",
       "16           russian  0.200952\n",
       "10             irish  0.185516\n",
       "19       west_indian  0.001717"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_risk_eth = []\n",
    "\n",
    "for code, name in eth_code.items():\n",
    "    av = find_risk(code, 'ethnicity', pred_prob)\n",
    "    new_row = {'eth': name, 'risk': av}\n",
    "    av_risk_eth.append(new_row)\n",
    "\n",
    "av_risk_eth_df = pd.DataFrame(av_risk_eth)\n",
    "av_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n",
    "\n",
    "av_risk_eth_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Abington MA US',\n",
       " 1: 'Acton MA US',\n",
       " 2: 'Acushnet MA US',\n",
       " 3: 'Adams MA US',\n",
       " 4: 'Agawam Town MA US',\n",
       " 5: 'Alford MA US',\n",
       " 6: 'Amesbury Town MA US',\n",
       " 7: 'Amherst MA US',\n",
       " 8: 'Andover MA US',\n",
       " 9: 'Arlington MA US',\n",
       " 10: 'Ashburnham MA US',\n",
       " 11: 'Ashby MA US',\n",
       " 12: 'Ashfield MA US',\n",
       " 13: 'Ashland MA US',\n",
       " 14: 'Athol MA US',\n",
       " 15: 'Attleboro MA US',\n",
       " 16: 'Auburn MA US',\n",
       " 17: 'Avon MA US',\n",
       " 18: 'Barnstable Town MA US',\n",
       " 19: 'Barre MA US',\n",
       " 20: 'Becket MA US',\n",
       " 21: 'Bedford MA US',\n",
       " 22: 'Bellingham MA US',\n",
       " 23: 'Belmont MA US',\n",
       " 24: 'Berkley MA US',\n",
       " 25: 'Beverly MA US',\n",
       " 26: 'Billerica MA US',\n",
       " 27: 'Boston MA US',\n",
       " 28: 'Bourne MA US',\n",
       " 29: 'Boxborough MA US',\n",
       " 30: 'Boxford MA US',\n",
       " 31: 'Boylston MA US',\n",
       " 32: 'Braintree Town MA US',\n",
       " 33: 'Brewster MA US',\n",
       " 34: 'Bridgewater MA US',\n",
       " 35: 'Brockton MA US',\n",
       " 36: 'Brookfield MA US',\n",
       " 37: 'Brookline MA US',\n",
       " 38: 'Burlington MA US',\n",
       " 39: 'Cambridge MA US',\n",
       " 40: 'Canton MA US',\n",
       " 41: 'Carlisle MA US',\n",
       " 42: 'Carver MA US',\n",
       " 43: 'Charlemont MA US',\n",
       " 44: 'Charlton MA US',\n",
       " 45: 'Chatham MA US',\n",
       " 46: 'Chelmsford MA US',\n",
       " 47: 'Chelsea MA US',\n",
       " 48: 'Chicopee MA US',\n",
       " 49: 'Chilmark MA US',\n",
       " 50: 'Clinton MA US',\n",
       " 51: 'Cohasset MA US',\n",
       " 52: 'Colrain MA US',\n",
       " 53: 'Concord MA US',\n",
       " 54: 'Dalton MA US',\n",
       " 55: 'Danvers MA US',\n",
       " 56: 'Dartmouth MA US',\n",
       " 57: 'Dedham MA US',\n",
       " 58: 'Deerfield MA US',\n",
       " 59: 'Dennis MA US',\n",
       " 60: 'Douglas MA US',\n",
       " 61: 'Dover MA US',\n",
       " 62: 'Dracut MA US',\n",
       " 63: 'Dudley MA US',\n",
       " 64: 'Duxbury MA US',\n",
       " 65: 'East Bridgewater MA US',\n",
       " 66: 'East Brookfield MA US',\n",
       " 67: 'East Longmeadow MA US',\n",
       " 68: 'Eastham MA US',\n",
       " 69: 'Easthampton Town MA US',\n",
       " 70: 'Easton MA US',\n",
       " 71: 'Everett MA US',\n",
       " 72: 'Fairhaven MA US',\n",
       " 73: 'Fall River MA US',\n",
       " 74: 'Falmouth MA US',\n",
       " 75: 'Fitchburg MA US',\n",
       " 76: 'Florida MA US',\n",
       " 77: 'Foxborough MA US',\n",
       " 78: 'Framingham MA US',\n",
       " 79: 'Franklin Town MA US',\n",
       " 80: 'Freetown MA US',\n",
       " 81: 'Gardner MA US',\n",
       " 82: 'Gill MA US',\n",
       " 83: 'Gloucester MA US',\n",
       " 84: 'Grafton MA US',\n",
       " 85: 'Granby MA US',\n",
       " 86: 'Granville MA US',\n",
       " 87: 'Great Barrington MA US',\n",
       " 88: 'Greenfield Town MA US',\n",
       " 89: 'Groton MA US',\n",
       " 90: 'Groveland MA US',\n",
       " 91: 'Hadley MA US',\n",
       " 92: 'Hamilton MA US',\n",
       " 93: 'Hampden MA US',\n",
       " 94: 'Hancock MA US',\n",
       " 95: 'Hanover MA US',\n",
       " 96: 'Hanson MA US',\n",
       " 97: 'Hardwick MA US',\n",
       " 98: 'Harwich MA US',\n",
       " 99: 'Haverhill MA US',\n",
       " 100: 'Hingham MA US',\n",
       " 101: 'Holbrook MA US',\n",
       " 102: 'Holden MA US',\n",
       " 103: 'Holliston MA US',\n",
       " 104: 'Holyoke MA US',\n",
       " 105: 'Hopedale MA US',\n",
       " 106: 'Hopkinton MA US',\n",
       " 107: 'Hudson MA US',\n",
       " 108: 'Hull MA US',\n",
       " 109: 'Huntington MA US',\n",
       " 110: 'Ipswich MA US',\n",
       " 111: 'Kingston MA US',\n",
       " 112: 'Lancaster MA US',\n",
       " 113: 'Lawrence MA US',\n",
       " 114: 'Lee MA US',\n",
       " 115: 'Leicester MA US',\n",
       " 116: 'Lenox MA US',\n",
       " 117: 'Leominster MA US',\n",
       " 118: 'Leverett MA US',\n",
       " 119: 'Lexington MA US',\n",
       " 120: 'Lincoln MA US',\n",
       " 121: 'Littleton MA US',\n",
       " 122: 'Longmeadow MA US',\n",
       " 123: 'Lowell MA US',\n",
       " 124: 'Ludlow MA US',\n",
       " 125: 'Lunenburg MA US',\n",
       " 126: 'Lynn MA US',\n",
       " 127: 'Lynnfield MA US',\n",
       " 128: 'Malden MA US',\n",
       " 129: 'Manchester-by-the-Sea MA US',\n",
       " 130: 'Mansfield MA US',\n",
       " 131: 'Marblehead MA US',\n",
       " 132: 'Marion MA US',\n",
       " 133: 'Marlborough MA US',\n",
       " 134: 'Marshfield MA US',\n",
       " 135: 'Mashpee MA US',\n",
       " 136: 'Mattapoisett MA US',\n",
       " 137: 'Maynard MA US',\n",
       " 138: 'Medfield MA US',\n",
       " 139: 'Medford MA US',\n",
       " 140: 'Medway MA US',\n",
       " 141: 'Melrose MA US',\n",
       " 142: 'Mendon MA US',\n",
       " 143: 'Merrimac MA US',\n",
       " 144: 'Methuen Town MA US',\n",
       " 145: 'Middleborough MA US',\n",
       " 146: 'Middleton MA US',\n",
       " 147: 'Milford MA US',\n",
       " 148: 'Millbury MA US',\n",
       " 149: 'Millis MA US',\n",
       " 150: 'Millville MA US',\n",
       " 151: 'Milton MA US',\n",
       " 152: 'Montague MA US',\n",
       " 153: 'Nantucket MA US',\n",
       " 154: 'Natick MA US',\n",
       " 155: 'Needham MA US',\n",
       " 156: 'New Bedford MA US',\n",
       " 157: 'New Braintree MA US',\n",
       " 158: 'New Marlborough MA US',\n",
       " 159: 'Newbury MA US',\n",
       " 160: 'Newburyport MA US',\n",
       " 161: 'Newton MA US',\n",
       " 162: 'Norfolk MA US',\n",
       " 163: 'North Adams MA US',\n",
       " 164: 'North Andover MA US',\n",
       " 165: 'North Attleborough MA US',\n",
       " 166: 'North Reading MA US',\n",
       " 167: 'Northampton MA US',\n",
       " 168: 'Northborough MA US',\n",
       " 169: 'Northbridge MA US',\n",
       " 170: 'Norton MA US',\n",
       " 171: 'Norwell MA US',\n",
       " 172: 'Norwood MA US',\n",
       " 173: 'Oak Bluffs MA US',\n",
       " 174: 'Orange MA US',\n",
       " 175: 'Orleans MA US',\n",
       " 176: 'Oxford MA US',\n",
       " 177: 'Palmer Town MA US',\n",
       " 178: 'Paxton MA US',\n",
       " 179: 'Peabody MA US',\n",
       " 180: 'Pembroke MA US',\n",
       " 181: 'Pepperell MA US',\n",
       " 182: 'Pittsfield MA US',\n",
       " 183: 'Plainville MA US',\n",
       " 184: 'Plymouth MA US',\n",
       " 185: 'Plympton MA US',\n",
       " 186: 'Quincy MA US',\n",
       " 187: 'Randolph MA US',\n",
       " 188: 'Raynham MA US',\n",
       " 189: 'Reading MA US',\n",
       " 190: 'Rehoboth MA US',\n",
       " 191: 'Revere MA US',\n",
       " 192: 'Rockland MA US',\n",
       " 193: 'Rockport MA US',\n",
       " 194: 'Rowley MA US',\n",
       " 195: 'Russell MA US',\n",
       " 196: 'Rutland MA US',\n",
       " 197: 'Salem MA US',\n",
       " 198: 'Salisbury MA US',\n",
       " 199: 'Saugus MA US',\n",
       " 200: 'Seekonk MA US',\n",
       " 201: 'Sharon MA US',\n",
       " 202: 'Shelburne MA US',\n",
       " 203: 'Sherborn MA US',\n",
       " 204: 'Shirley MA US',\n",
       " 205: 'Shrewsbury MA US',\n",
       " 206: 'Somerset MA US',\n",
       " 207: 'Somerville MA US',\n",
       " 208: 'South Hadley MA US',\n",
       " 209: 'Southampton MA US',\n",
       " 210: 'Southborough MA US',\n",
       " 211: 'Southbridge Town MA US',\n",
       " 212: 'Spencer MA US',\n",
       " 213: 'Springfield MA US',\n",
       " 214: 'Stoneham MA US',\n",
       " 215: 'Stoughton MA US',\n",
       " 216: 'Stow MA US',\n",
       " 217: 'Sturbridge MA US',\n",
       " 218: 'Sudbury MA US',\n",
       " 219: 'Swampscott MA US',\n",
       " 220: 'Swansea MA US',\n",
       " 221: 'Taunton MA US',\n",
       " 222: 'Templeton MA US',\n",
       " 223: 'Tewksbury MA US',\n",
       " 224: 'Tisbury MA US',\n",
       " 225: 'Topsfield MA US',\n",
       " 226: 'Townsend MA US',\n",
       " 227: 'Tyngsborough MA US',\n",
       " 228: 'Tyringham MA US',\n",
       " 229: 'Wakefield MA US',\n",
       " 230: 'Walpole MA US',\n",
       " 231: 'Waltham MA US',\n",
       " 232: 'Ware MA US',\n",
       " 233: 'Wareham MA US',\n",
       " 234: 'Warren MA US',\n",
       " 235: 'Warwick MA US',\n",
       " 236: 'Watertown Town MA US',\n",
       " 237: 'Webster MA US',\n",
       " 238: 'Wellesley MA US',\n",
       " 239: 'Wenham MA US',\n",
       " 240: 'West Brookfield MA US',\n",
       " 241: 'West Newbury MA US',\n",
       " 242: 'West Springfield Town MA US',\n",
       " 243: 'West Stockbridge MA US',\n",
       " 244: 'West Tisbury MA US',\n",
       " 245: 'Westborough MA US',\n",
       " 246: 'Westfield MA US',\n",
       " 247: 'Westford MA US',\n",
       " 248: 'Westhampton MA US',\n",
       " 249: 'Westminster MA US',\n",
       " 250: 'Weston MA US',\n",
       " 251: 'Westport MA US',\n",
       " 252: 'Westwood MA US',\n",
       " 253: 'Weymouth Town MA US',\n",
       " 254: 'Whitman MA US',\n",
       " 255: 'Wilbraham MA US',\n",
       " 256: 'Williamstown MA US',\n",
       " 257: 'Wilmington MA US',\n",
       " 258: 'Winchendon MA US',\n",
       " 259: 'Winchester MA US',\n",
       " 260: 'Winthrop Town MA US',\n",
       " 261: 'Woburn MA US',\n",
       " 262: 'Worcester MA US',\n",
       " 263: 'Wrentham MA US',\n",
       " 264: 'Yarmouth MA US'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 3:\n",
    "Springfield: 213 - 5\n",
    "Lawrence: 113 - 3\n",
    "Holyoke: 104 - 2\n",
    "\n",
    "Top 3:\n",
    "Dover: 16 - 2\n",
    "Lexington: 119 - 0\n",
    "Wellesley: 238 - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep taking people from the top and bottom until its 20 on each side, then find av risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "richTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n",
    "                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n",
    "                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n",
    "                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n",
    "                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\"]\n",
    "\n",
    "poorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n",
    "    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n",
    "    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trouble appending town names to dataframe with town codes and people counts\n",
    "\n",
    "need that so that we can then get the top twenty of each \n",
    "to then average to get rich likelihood and poor likelihood of diabetes \n",
    "yayyyy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (144, 1), indices imply (144, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m townCounts \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirthplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m----> 3\u001b[0m townCounts_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtownCounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbirthplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m townName \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirthplace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m currBP \u001b[38;5;129;01min\u001b[39;00m richTowns: \n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/frame.py:816\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    805\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    806\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    814\u001b[0m         )\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    334\u001b[0m )\n\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-0451/lib/python3.9/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (144, 1), indices imply (144, 2)"
     ]
    }
   ],
   "source": [
    "townCounts = X_test.groupby('birthplace').size()\n",
    "\n",
    "townCounts_df = pd.DataFrame(townCounts, columns=['birthplace', 'Count'])\n",
    "\n",
    "townName = pd.DataFrame(columns=['name', 'birthplace', 'pop'])\n",
    "\n",
    "for currBP in richTowns: \n",
    "    bp_code_swapped = {value: key for key, value in bp_code.items()}\n",
    "    town = currBP + ' MA US'\n",
    "    code = bp_code_swapped[town]\n",
    "\n",
    "    count = townCounts_df.loc[townCounts_df['birthplace'] == code, 'Count'].iloc[0]\n",
    "    new_row = {'name': town, 'birthplace': code, 'pop': count}\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    print(new_row_df)\n",
    "    townName = pd.concat([townName, new_row_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Count'], dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "townCounts_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proceed with the following part to get top 20 people from each rich and poor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m richTownsUsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      2\u001b[0m peopleCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m personRow, town \u001b[38;5;129;01min\u001b[39;00m poorTowns:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m peopleCount \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m: \n\u001b[1;32m      5\u001b[0m         richTownsUsed\u001b[38;5;241m.\u001b[39madd(town)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# richTownsUsed = set()\n",
    "# peopleCount = 0\n",
    "# for personRow, town in poorTowns:\n",
    "#     while peopleCount <= 20: \n",
    "#         richTownsUsed.add(town)\n",
    "#         peopleCount += 1\n",
    "#         poorTownsUsed = set()\n",
    "# peopleCount = 0\n",
    "# for personRow, town in poorTowns:\n",
    "#     while peopleCount <= 20: \n",
    "#         poorTownsUsed.add(town)\n",
    "#         peopleCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "springfield = find_risk(213, 'birthplace', pred_prob)\n",
    "lawrence = find_risk(113, 'birthplace', pred_prob)\n",
    "holyoke = find_risk(104, 'birthplace', pred_prob)\n",
    "\n",
    "dover = find_risk(16, 'birthplace', pred_prob)\n",
    "lexington = find_risk(119, 'birthplace', pred_prob)\n",
    "wellesley = find_risk(238, 'birthplace', pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (X_test['birthplace'] == 238)\n",
    "prob_subset = pred_prob[indices]\n",
    "\n",
    "len(prob_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0984442595290341 \n",
      " 0.3341135562139574 \n",
      " 0.001060267857142857 \n",
      " 0.5650708866146601 \n",
      " nan \n",
      " 0.7997913691073069 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(springfield, \"\\n\", lawrence, \"\\n\", holyoke, \"\\n\", dover, \"\\n\", lexington, \"\\n\", wellesley, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
